{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.4.1'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "scipy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import os\n",
    "import os.path\n",
    "import joblib\n",
    "from Datasets import *\n",
    "from Settings import *\n",
    "from Estimators import *\n",
    "from Metrics import *\n",
    "from Policy import *\n",
    "import scipy.stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................................\n",
      "Datasets:loadTxt [INFO] Loaded /Users/haohongxu/slates_semisynth_expts/MSLR-WEB10K/Fold1/train.txt \t NumQueries 6000 \t [Min/Max]DocsPerQuery 1 809\n"
     ]
    }
   ],
   "source": [
    "for foldID in range(1,2):\n",
    "    for fraction in ['train']:\n",
    "        mslrData=Datasets()\n",
    "        mslrData.loadTxt(Settings.DATA_DIR+'MSLR-WEB10K/Fold'+str(foldID)+'/'+fraction+'.txt', 'MSLR10k-'+str(foldID)+'-'+fraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=mslrData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Datasets.Datasets at 0x7fc1d59911c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchorURLFeatures, bodyTitleDocFeatures=Settings.get_feature_sets(\"MSLR10k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "class Policy:\n",
    "    #dataset: (Datasets) Must be initialized using Datasets.loadTxt(...)/loadNpz(...)\n",
    "    #allow_repetitions: (bool) If true, the policy predicts rankings with repeated documents\n",
    "    def __init__(self, dataset, allow_repetitions):\n",
    "        self.dataset=dataset\n",
    "        self.allowRepetitions=allow_repetitions\n",
    "        self.name=None\n",
    "        ###All sub-classes of Policy should supply a predict method\n",
    "        ###Requires: (int) query_id; (int) ranking_size.\n",
    "        ###Returns: list[int],length=min(ranking_size,docsPerQuery[query_id]) ranking\n",
    "class DeterministicPolicy(Policy):\n",
    "    #model_type: (str) Model class to use for scoring documents\n",
    "    def __init__(self, dataset, model_type, regress_gains=False, weighted_ls=False, hyper_params=None):\n",
    "        Policy.__init__(self, dataset, False)\n",
    "        self.modelType=model_type\n",
    "        self.hyperParams={'alpha': (numpy.logspace(-3,2,num=6,base=10)).tolist()}\n",
    "        if hyper_params is not None:\n",
    "            self.hyperParams=hyper_params\n",
    "        \n",
    "        self.regressGains=regress_gains\n",
    "        self.weighted=weighted_ls\n",
    "        \n",
    "        self.treeDepths={'max_depth': list(range(3,21,3))}\n",
    "        \n",
    "        #Must call train(...) to set all these members\n",
    "        #before using DeterministicPolicy objects elsewhere\n",
    "        self.featureList=None\n",
    "        if self.modelType=='tree':\n",
    "            self.tree=None\n",
    "        else:\n",
    "            self.policyParams=None\n",
    "            \n",
    "        #These members are set by predictAll(...) method\n",
    "        self.savedRankingsSize=None\n",
    "        self.savedRankings=None\n",
    "        \n",
    "        print(\"DeterministicPolicy:init [INFO] Dataset\", dataset.name, flush=True)\n",
    "    \n",
    "    #feature_list: list[int],length=unmaskedFeatures; List of features that should be used for training\n",
    "    #name: (str) String to help identify this DeterministicPolicy object henceforth\n",
    "    def train(self, feature_list, name):\n",
    "        self.featureList=feature_list\n",
    "        self.name=name+'-'+self.modelType\n",
    "        modelFile=Settings.DATA_DIR+self.dataset.name+'_'+self.name\n",
    "        if 'alpha' not in self.hyperParams:\n",
    "            #Expecting hyper-params for GBRT; Add those hyper-params to the model file name\n",
    "            modelFile=modelFile+'ensemble-'+str(self.hyperParams['ensemble'])+'_lr-'+str(self.hyperParams['lr'])+'_subsample-'+str(self.hyperParams['subsample'])+'_leaves-'+str(self.hyperParams['leaves'])\n",
    "            \n",
    "        if self.modelType=='tree' or self.modelType=='gbrt':\n",
    "            modelFile+='.z'\n",
    "        else:\n",
    "            modelFile+='.npz'\n",
    "            \n",
    "        self.savedRankingsSize=None\n",
    "        self.savedRankings=None\n",
    "        \n",
    "        if os.path.exists(modelFile):\n",
    "            if self.modelType=='tree' or self.modelType=='gbrt':\n",
    "                self.tree=joblib.load(modelFile)\n",
    "                print(\"DeterministicPolicy:train [INFO] Using precomputed policy\", modelFile, flush=True)\n",
    "            else:\n",
    "                with numpy.load(modelFile) as npFile:\n",
    "                    self.policyParams=npFile['policyParams']\n",
    "                print(\"DeterministicPolicy:train [INFO] Using precomputed policy\", modelFile, flush=True)\n",
    "                print(\"DeterministicPolicy:train [INFO] PolicyParams\", self.policyParams,flush=True)\n",
    "        else:\n",
    "            numQueries=len(self.dataset.features)\n",
    "        \n",
    "            allFeatures=None\n",
    "            allTargets=None\n",
    "            print(\"DeterministicPolicy:train [INFO] Constructing features and targets\", flush=True)\n",
    "                \n",
    "            if self.dataset.mask is None:\n",
    "                allFeatures=scipy.sparse.vstack(self.dataset.features, format='csc')\n",
    "                allTargets=numpy.hstack(self.dataset.relevances)\n",
    "            else:\n",
    "                temporaryFeatures=[]\n",
    "                temporaryTargets=[]\n",
    "                for currentQuery in range(numQueries):\n",
    "                    temporaryFeatures.append(self.dataset.features[currentQuery][self.dataset.mask[currentQuery], :])\n",
    "                    temporaryTargets.append(self.dataset.relevances[currentQuery][self.dataset.mask[currentQuery]])\n",
    "                \n",
    "                allFeatures=scipy.sparse.vstack(temporaryFeatures, format='csc')\n",
    "                allTargets=numpy.hstack(temporaryTargets)\n",
    "        \n",
    "            if self.regressGains:\n",
    "                allTargets=numpy.exp2(allTargets)-1.0\n",
    "            \n",
    "            allSampleWeights=None\n",
    "            fitParams=None\n",
    "            if self.weighted:\n",
    "                allSampleWeights=numpy.array(self.dataset.docsPerQuery, dtype=numpy.float64)\n",
    "                allSampleWeights=numpy.reciprocal(allSampleWeights)\n",
    "                allSampleWeights=numpy.repeat(allSampleWeights, self.dataset.docsPerQuery)    \n",
    "                fitParams={'sample_weight': allSampleWeights}\n",
    "            \n",
    "            #Restrict features to only the unmasked features\n",
    "            if self.featureList is not None:\n",
    "                print(\"DeterministicPolicy:train [INFO] Masking unused features. Remaining feature size\", \n",
    "                    len(feature_list), flush=True)\n",
    "                allFeatures = allFeatures[:, self.featureList]\n",
    "        \n",
    "            print(\"DeterministicPolicy:train [INFO] Beginning training\", self.modelType, flush=True)\n",
    "            if self.modelType=='tree':\n",
    "                treeCV=sklearn.model_selection.GridSearchCV(sklearn.tree.DecisionTreeRegressor(criterion=\"mse\",\n",
    "                                                        splitter=\"random\", min_samples_split=4, \n",
    "                                                        min_samples_leaf=4),\n",
    "                                param_grid=self.treeDepths,\n",
    "                                scoring=None, n_jobs=-2,\n",
    "                                cv=5, refit=True, verbose=0, pre_dispatch=\"1*n_jobs\",\n",
    "                                error_score='raise', return_train_score=False)\n",
    "                            \n",
    "                treeCV.fit(allFeatures, allTargets)\n",
    "                self.tree=treeCV.best_estimator_\n",
    "                print(\"DeterministicPolicy:train [INFO] Done. Best depth\", \n",
    "                            treeCV.best_params_['max_depth'], flush=True)\n",
    "                joblib.dump(self.tree, modelFile, compress=9, protocol=-1)\n",
    "            \n",
    "            elif self.modelType=='lasso':\n",
    "                lassoCV=sklearn.model_selection.GridSearchCV(sklearn.linear_model.Lasso(fit_intercept=False,\n",
    "                                                        normalize=False, precompute=False, copy_X=False, \n",
    "                                                        max_iter=3000, tol=1e-4, warm_start=False, positive=False,\n",
    "                                                        random_state=None, selection='random'),\n",
    "                                param_grid=self.hyperParams,\n",
    "                                scoring=None, n_jobs=-2,\n",
    "                                cv=5, refit=True, verbose=0, pre_dispatch=\"1*n_jobs\",\n",
    "                                error_score='raise', return_train_score=False)\n",
    "                                \n",
    "                lassoCV.fit(allFeatures, allTargets)\n",
    "                self.policyParams=lassoCV.best_estimator_.coef_\n",
    "                print(\"DeterministicPolicy:train [INFO] Done. CVAlpha\", lassoCV.best_params_['alpha'], flush=True)\n",
    "                print(\"DeterministicPolicy:train [INFO] PolicyParams\", self.policyParams,flush=True)\n",
    "                numpy.savez_compressed(modelFile, policyParams=self.policyParams)\n",
    "        \n",
    "            elif self.modelType == 'ridge':\n",
    "                ridgeCV=sklearn.model_selection.GridSearchCV(sklearn.linear_model.Ridge(fit_intercept=False,\n",
    "                                                                                    normalize=False, copy_X=False,\n",
    "                                                                                    max_iter=3000, tol=1e-4, random_state=None),\n",
    "                                                         param_grid=self.hyperParams,\n",
    "                                                         n_jobs=-2, \n",
    "                                                         cv=3, refit=True, verbose=0, pre_dispatch='1*n_jobs')\n",
    "                ridgeCV.fit(allFeatures, allTargets)\n",
    "                self.policyParams=ridgeCV.best_estimator_.coef_\n",
    "                print(\"DeterministicPolicy:train [INFO] Done. CVAlpha\", ridgeCV.best_params_['alpha'], flush=True)\n",
    "            elif self.modelType=='gbrt':\n",
    "                tree=sklearn.ensemble.GradientBoostingRegressor(learning_rate=self.hyperParams['lr'],\n",
    "                            n_estimators=self.hyperParams['ensemble'], subsample=self.hyperParams['subsample'], max_leaf_nodes=self.hyperParams['leaves'], \n",
    "                            max_features=1.0)\n",
    "                tree.fit(allFeatures, allTargets, sample_weight=allSampleWeights)\n",
    "                self.tree=tree\n",
    "                print(\"DeterministicPolicy:train [INFO] Done.\", flush=True)\n",
    "                joblib.dump(self.tree, modelFile, compress=9, protocol=-1)\n",
    "            \n",
    "            else:\n",
    "                print(\"DeterministicPolicy:train [ERR] %s not supported.\" % self.modelType, flush=True)\n",
    "                sys.exit(0)\n",
    "    \n",
    "    #query_id: (int) Query ID in self.dataset\n",
    "    #ranking_size: (int) Size of ranking. Returned ranking length is min(ranking_size,docsPerQuery[query_id])\n",
    "    #                       Use ranking_size=-1 to rank all available documents for query_id\n",
    "    def predict(self, query_id, ranking_size):\n",
    "        if self.savedRankingsSize is not None and self.savedRankingsSize==ranking_size:\n",
    "            return self.savedRankings[query_id]\n",
    "        \n",
    "        allowedDocs=self.dataset.docsPerQuery[query_id]\n",
    "        validDocs=ranking_size\n",
    "        if ranking_size <= 0 or validDocs > allowedDocs:\n",
    "            validDocs=allowedDocs\n",
    "        \n",
    "        currentFeatures=None\n",
    "        if self.dataset.mask is None:\n",
    "            if self.featureList is not None:\n",
    "                currentFeatures=self.dataset.features[query_id][:, self.featureList]\n",
    "            else:\n",
    "                currentFeatures=self.dataset.features[query_id]\n",
    "            \n",
    "        else:\n",
    "            currentFeatures=self.dataset.features[query_id][self.dataset.mask[query_id], :]\n",
    "            if self.featureList is not None:\n",
    "                currentFeatures=currentFeatures[:, self.featureList]\n",
    "        \n",
    "        allDocScores=None\n",
    "        if self.modelType=='tree':\n",
    "            allDocScores=self.tree.predict(currentFeatures)\n",
    "        elif self.modelType=='gbrt':\n",
    "            allDocScores=self.tree.predict(currentFeatures.toarray())\n",
    "        else:\n",
    "            allDocScores=currentFeatures.dot(self.policyParams)\n",
    "            \n",
    "        tieBreaker=numpy.random.random(allDocScores.size)\n",
    "        sortedDocScores=numpy.lexsort((tieBreaker,-allDocScores))[0:validDocs]\n",
    "        if self.dataset.mask is None:\n",
    "            return sortedDocScores\n",
    "        else:\n",
    "            return self.dataset.mask[query_id][sortedDocScores]\n",
    "    \n",
    "    #ranking_size: (int) Size of ranking. Returned ranking length is min(ranking_size,docsPerQuery[query_id])\n",
    "    #                       Use ranking_size=-1 to rank all available documents for query_id\n",
    "    def predictAll(self, ranking_size):\n",
    "        if self.savedRankingsSize is not None and self.savedRankingsSize==ranking_size:\n",
    "            return\n",
    "            \n",
    "        numQueries=len(self.dataset.features)\n",
    "        predictedRankings=[]\n",
    "        for i in range(numQueries):\n",
    "            predictedRankings.append(self.predict(i, ranking_size))\n",
    "                \n",
    "            if i%100==0:\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "                \n",
    "        self.savedRankingsSize=ranking_size\n",
    "        self.savedRankings=predictedRankings\n",
    "        print(\"\", flush=True)\n",
    "        print(\"DeterministicPolicy:predictAll [INFO] Generated all predictions for %s using policy: \" %\n",
    "                self.dataset.name, self.name, flush=True)\n",
    "        \n",
    "    #num_allowed_docs: (int) Filters the dataset where the max docs per query is num_allowed_docs.\n",
    "    #                        Uses policyParams to rank and filter the original document set.\n",
    "    def filterDataset(self, num_allowed_docs):\n",
    "        self.savedRankingsSize=None\n",
    "        self.savedRankings=None\n",
    "        \n",
    "        numQueries=len(self.dataset.docsPerQuery)\n",
    "        \n",
    "        self.dataset.name=self.dataset.name+'-filt('+self.name+'-'+str(num_allowed_docs)+')'\n",
    "        \n",
    "        newMask = []\n",
    "        for i in range(numQueries):\n",
    "            producedRanking=self.predict(i, num_allowed_docs)\n",
    "            self.dataset.docsPerQuery[i]=numpy.shape(producedRanking)[0]\n",
    "            newMask.append(producedRanking)\n",
    "            if i%100==0:\n",
    "                print(\".\", end=\"\", flush=True)\n",
    "                \n",
    "        self.dataset.mask=newMask\n",
    "        print(\"\", flush=True)\n",
    "        print(\"DeterministicPolicy:filteredDataset [INFO] New Name\", self.dataset.name, \"\\t MaxNumDocs\", num_allowed_docs, flush=True)\n",
    "\n",
    "class UniformPolicy(Policy):\n",
    "    def __init__(self, dataset, allow_repetitions):\n",
    "        Policy.__init__(self, dataset, allow_repetitions)\n",
    "        self.name='Unif-'\n",
    "        if allow_repetitions:\n",
    "            self.name+='Rep'\n",
    "        else:\n",
    "            self.name+='NoRep'\n",
    "    \n",
    "        #These members are set on-demand by setupGamma(...)\n",
    "        self.gammas=None\n",
    "        self.gammaRankingSize=None\n",
    "        \n",
    "        print(\"UniformPolicy:init [INFO] Dataset: %s AllowRepetitions:\" % dataset.name,\n",
    "                        allow_repetitions, flush=True)\n",
    "    \n",
    "    #ranking_size: (int) Size of ranking.\n",
    "    def setupGamma(self, ranking_size):\n",
    "        if self.gammaRankingSize is not None and self.gammaRankingSize==ranking_size:\n",
    "            print(\"UniformPolicy:setupGamma [INFO] Gamma has been pre-computed for this ranking_size. Size of Gamma cache:\", len(self.gammas), flush=True)\n",
    "            return\n",
    "        \n",
    "        gammaFile=Settings.DATA_DIR+self.dataset.name+'_'+self.name+'_'+str(ranking_size)+'.z'\n",
    "        if os.path.exists(gammaFile):\n",
    "            self.gammas=joblib.load(gammaFile)\n",
    "            self.gammaRankingSize=ranking_size\n",
    "            print(\"UniformPolicy:setupGamma [INFO] Using precomputed gamma\", gammaFile, flush=True)\n",
    "            \n",
    "        else:\n",
    "            self.gammas={}\n",
    "            self.gammaRankingSize=ranking_size\n",
    "            \n",
    "            candidateSet=set(self.dataset.docsPerQuery)\n",
    "            \n",
    "            responses=joblib.Parallel(n_jobs=-2, verbose=50)(joblib.delayed(UniformGamma)(i, ranking_size, self.allowRepetitions) for i in candidateSet)\n",
    "            \n",
    "            for tup in responses:\n",
    "                self.gammas[tup[0]]=tup[1]\n",
    "            \n",
    "            joblib.dump(self.gammas, gammaFile, compress=9, protocol=-1)\n",
    "            print(\"\", flush=True)\n",
    "            print(\"UniformPolicy:setupGamma [INFO] Finished creating Gamma_pinv cache. Size\", len(self.gammas), flush=True)\n",
    "\n",
    "    def predict(self, query_id, ranking_size):\n",
    "        allowedDocs=self.dataset.docsPerQuery[query_id]    \n",
    "        \n",
    "        validDocs=ranking_size\n",
    "        if ranking_size < 0 or ((not self.allowRepetitions) and (validDocs > allowedDocs)):\n",
    "            validDocs=allowedDocs\n",
    "            \n",
    "        producedRanking=None\n",
    "        if self.allowRepetitions:\n",
    "            producedRanking=numpy.random.choice(allowedDocs, size=validDocs,\n",
    "                                replace=True)\n",
    "        else:\n",
    "            producedRanking=numpy.random.choice(allowedDocs, size=validDocs,\n",
    "                                replace=False)\n",
    "                                \n",
    "        if self.dataset.mask is None:\n",
    "            return producedRanking\n",
    "        else:\n",
    "            return self.dataset.mask[query_id][producedRanking]\n",
    "        \n",
    "\n",
    "class NonUniformPolicy(Policy):\n",
    "    def __init__(self, deterministic_policy, dataset, allow_repetitions, decay):\n",
    "        Policy.__init__(self, dataset, allow_repetitions)\n",
    "        self.decay = decay\n",
    "        self.policy = deterministic_policy\n",
    "        self.name='NonUnif-'\n",
    "        if allow_repetitions:\n",
    "            self.name+='Rep'\n",
    "        else:\n",
    "            self.name+='NoRep'\n",
    "        self.name += '(' + deterministic_policy.name + ';' + str(decay) + ')'\n",
    "        \n",
    "        #These members are set on-demand by setupGamma\n",
    "        self.gammas=None\n",
    "        self.multinomials=None\n",
    "        self.gammaRankingSize=None\n",
    "        \n",
    "        print(\"NonUniformPolicy:init [INFO] Dataset: %s AllowRepetitions:\" % dataset.name,\n",
    "                        allow_repetitions, \"\\t Decay:\", decay, flush=True)\n",
    "    \n",
    "    \n",
    "    def setupGamma(self, ranking_size):\n",
    "        if self.gammaRankingSize is not None and self.gammaRankingSize==ranking_size:\n",
    "            print(\"NonUniformPolicy:setupGamma [INFO] Gamma has been pre-computed for this ranking_size. Size of Gamma cache:\", len(self.gammas), flush=True)\n",
    "            return\n",
    "        \n",
    "        gammaFile=Settings.DATA_DIR+self.dataset.name+'_'+self.name+'_'+str(ranking_size)+'.z'\n",
    "        if os.path.exists(gammaFile):\n",
    "            self.gammas, self.multinomials=joblib.load(gammaFile)\n",
    "            self.gammaRankingSize=ranking_size\n",
    "            print(\"NonUniformPolicy:setupGamma [INFO] Using precomputed gamma\", gammaFile, flush=True)\n",
    "            \n",
    "        else:\n",
    "            self.gammas={}\n",
    "            self.multinomials={}\n",
    "            self.gammaRankingSize=ranking_size\n",
    "            \n",
    "            candidateSet=set(self.dataset.docsPerQuery)\n",
    "            responses=joblib.Parallel(n_jobs=-2, verbose=50)(joblib.delayed(NonUniformGamma)(i, self.decay, ranking_size, self.allowRepetitions) for i in candidateSet)\n",
    "            \n",
    "            for tup in responses:\n",
    "                self.gammas[tup[0]]=tup[2]\n",
    "                self.multinomials[tup[0]]=tup[1]\n",
    "            \n",
    "            joblib.dump((self.gammas, self.multinomials), gammaFile, compress=9, protocol=-1)\n",
    "            print(\"\", flush=True)\n",
    "            print(\"NonUniformPolicy:setupGamma [INFO] Finished creating Gamma_pinv cache. Size\", len(self.gammas), flush=True)\n",
    "\n",
    "        self.policy.predictAll(-1)\n",
    "\n",
    "    def predict(self, query_id, ranking_size):\n",
    "        allowedDocs=self.dataset.docsPerQuery[query_id]    \n",
    "        underlyingRanking=self.policy.predict(query_id, -1)\n",
    "            \n",
    "        validDocs=ranking_size\n",
    "        if ranking_size < 0 or ((not self.allowRepetitions) and (validDocs > allowedDocs)):\n",
    "            validDocs=allowedDocs\n",
    "            \n",
    "        currentDistribution=self.multinomials[allowedDocs]\n",
    "        producedRanking=None\n",
    "        if self.allowRepetitions:\n",
    "            producedRanking=numpy.random.choice(allowedDocs, size=validDocs,\n",
    "                                replace=True, p=currentDistribution)\n",
    "        else:\n",
    "            producedRanking=numpy.random.choice(allowedDocs, size=validDocs,\n",
    "                                replace=False, p=currentDistribution)\n",
    "                                \n",
    "        return underlyingRanking[producedRanking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeterministicPolicy:init [INFO] Dataset MSLR10k-1-train\n",
      "DeterministicPolicy:train [INFO] Constructing features and targets\n",
      "DeterministicPolicy:train [INFO] Masking unused features. Remaining feature size 57\n",
      "DeterministicPolicy:train [INFO] Beginning training tree\n",
      "DeterministicPolicy:train [INFO] Done. Best depth 9\n",
      "............................................................\n",
      "DeterministicPolicy:filteredDataset [INFO] New Name MSLR10k-1-train-filt(url-tree-100) \t MaxNumDocs 100\n"
     ]
    }
   ],
   "source": [
    "import imp\n",
    "imp.reload(Policy)\n",
    "numpy.random.seed(args.numpy_seed)\n",
    "detLogger=Policy.DeterministicPolicy(data, 'tree')\n",
    "detLogger.train(anchorURLFeatures, 'url')\n",
    "\n",
    "detLogger.filterDataset(args.max_docs)\n",
    "data=detLogger.dataset\n",
    "del detLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Datasets\n",
    "import argparse\n",
    "import Settings\n",
    "import sys\n",
    "import os\n",
    "import numpy\n",
    "import Policy\n",
    "import Metrics\n",
    "import Estimators\n",
    "import joblib\n",
    "parser = argparse.ArgumentParser(description='Synthetic Testbed Experiments.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser= argparse.ArgumentParser(description='Synthetic Testbed Experiments.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['--stop'], dest='stop', nargs=None, const=None, default=10, type=<class 'int'>, choices=None, help='Stopping iteration number', metavar=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.add_argument('--max_docs', '-m', metavar='M', type=int, help='Filter documents',\n",
    "                        default=100)\n",
    "parser.add_argument('--length_ranking', '-l', metavar='L', type=int, help='Ranking Size',\n",
    "                    default=10)\n",
    "parser.add_argument('--replacement', '-r', metavar='R', type=bool, help='Sampling with or without replacement',\n",
    "                    default=False)\n",
    "parser.add_argument('--temperature', '-t', metavar='T', type=float, help='Temperature for logging policy', \n",
    "                    default=1.0)        #Use 0 < temperature < 2 to have reasonable tails for logger [-t 2 => smallest prob is 10^-4 (Uniform is 10^-2)]\n",
    "parser.add_argument('--logging_ranker', '-f', metavar='F', type=str, help='Model for logging ranker', \n",
    "                    default=\"tree\", choices=[\"tree\", \"lasso\"])\n",
    "parser.add_argument('--evaluation_ranker', '-e', metavar='E', type=str, help='Model for evaluation ranker', \n",
    "                    default=\"lasso\", choices=[\"tree\", \"lasso\"])\n",
    "parser.add_argument('--dataset', '-d', metavar='D', type=str, help='Which dataset to use',\n",
    "                    default=\"MSLR10k\", choices=[\"MSLR\", \"MSLR10k\", \"MQ2008\", \"MQ2007\"])\n",
    "parser.add_argument('--value_metric', '-v', metavar='V', type=str, help='Which metric to evaluate',\n",
    "                    default=\"ERR\", choices=[\"NDCG\", \"ERR\", \"MaxRel\", \"SumRel\"])\n",
    "parser.add_argument('--numpy_seed', '-n', metavar='N', type=int, \n",
    "                    help='Seed for numpy.random', default=387)\n",
    "parser.add_argument('--output_dir', '-o', metavar='O', type=str, \n",
    "                    help='Directory to store pkls', default=Settings.DATA_DIR)\n",
    "parser.add_argument('--approach', '-a', metavar='A', type=str, \n",
    "                    help='Approach name', default='IPS', choices=[\"OnPolicy\", \"IPS\", \"IPS_SN\", \"PI\", \"PI_SN\", \"DM_tree\", \"DM_lasso\", \"DMc_lasso\", \"DM_ridge\", \"DMc_ridge\"])\n",
    "parser.add_argument('--logSize', '-s', metavar='S', type=int, \n",
    "                    help='Size of log', default=30000)\n",
    "parser.add_argument('--trainingSize', '-z', metavar='Z', type=int, \n",
    "                    help='Size of training data for direct estimators', default=100)\n",
    "parser.add_argument('--saveSize', '-u', metavar='U', type=int, \n",
    "                    help='Number of saved datapoints', default=10000)\n",
    "parser.add_argument('--start', type=int,\n",
    "                    help='Starting iteration number', default=1)\n",
    "parser.add_argument('--stop', type=int,\n",
    "                    help='Stopping iteration number', default=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Datasets.Datasets at 0x7fc1d59911c0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeterministicPolicy:init [INFO] Dataset MSLR10k-1-train-filt(url-tree-100)\n",
      "DeterministicPolicy:train [INFO] Using precomputed policy /Users/haohongxu/slates_semisynth_expts/MSLR10k-1-train-filt(url-tree-100)_url-tree.z\n",
      "NonUniformPolicy:init [INFO] Dataset: MSLR10k-1-train-filt(url-tree-100) AllowRepetitions: False \t Decay: 1.0\n",
      "DeterministicPolicy:init [INFO] Dataset MSLR10k-1-train-filt(url-tree-100)\n",
      "DeterministicPolicy:train [INFO] Using precomputed policy /Users/haohongxu/slates_semisynth_expts/MSLR10k-1-train-filt(url-tree-100)_url-tree.z\n",
      "NonUniformPolicy:init [INFO] Dataset: MSLR10k-1-train-filt(url-tree-100) AllowRepetitions: False \t Decay: 1.0\n",
      "NonUniformPolicy:setupGamma [INFO] Using precomputed gamma /Users/haohongxu/slates_semisynth_expts/MSLR10k-1-train-filt(url-tree-100)_NonUnif-NoRep(url-tree;1.0)_10.z\n",
      "............................................................\n",
      "DeterministicPolicy:predictAll [INFO] Generated all predictions for MSLR10k-1-train-filt(url-tree-100) using policy:  url-tree\n",
      "NonUniformPolicy:setupGamma [INFO] Using precomputed gamma /Users/haohongxu/slates_semisynth_expts/MSLR10k-1-train-filt(url-tree-100)_NonUnif-NoRep(url-tree;1.0)_10.z\n",
      "............................................................\n",
      "DeterministicPolicy:predictAll [INFO] Generated all predictions for MSLR10k-1-train-filt(url-tree-100) using policy:  url-tree\n",
      "Parallel:main [LOG] Temperature: 1.0 \t Smallest marginal probability: 0.0023752969121140144\n"
     ]
    }
   ],
   "source": [
    "##import imp\n",
    "##imp.reload(Policy)\n",
    "#Setup target policy\n",
    "numpy.random.seed(args.numpy_seed)\n",
    "##targetPolicy=DeterministicPolicy(data, 'tree')\n",
    "##targetPolicy.train(bodyTitleDocFeatures, 'body')\n",
    "##targetPolicy.predictAll(args.length_ranking)\n",
    "\n",
    "loggingPolicy=None\n",
    "if args.temperature <= 0.0:\n",
    "    loggingPolicy=UniformPolicy(data, args.replacement)\n",
    "\n",
    "else:\n",
    "    underlyingPolicy=Policy.DeterministicPolicy(data, args.logging_ranker)\n",
    "    underlyingPolicy.train(anchorURLFeatures, 'url')\n",
    "    loggingPolicy=NonUniformPolicy(underlyingPolicy, data, args.replacement, args.temperature)\n",
    "    \n",
    "    \n",
    "    ## traget\n",
    "    underlyingPolicy2=Policy.DeterministicPolicy(data, args.logging_ranker)\n",
    "    underlyingPolicy2.train(anchorURLFeatures, 'url')\n",
    "    targetPolicy=NonUniformPolicy(underlyingPolicy2, data, args.replacement, args.temperature)\n",
    "\n",
    "\n",
    "\n",
    "loggingPolicy.setupGamma(args.length_ranking)\n",
    "targetPolicy.setupGamma(args.length_ranking)\n",
    "\n",
    "smallestProb=1.0\n",
    "docSet=set(data.docsPerQuery)\n",
    "for i in docSet:\n",
    "    currentMin=None\n",
    "    if args.temperature > 0.0:\n",
    "        currentMin=numpy.amin(loggingPolicy.multinomials[i])\n",
    "    else:\n",
    "        currentMin=1.0/i\n",
    "    if currentMin < smallestProb:\n",
    "        smallestProb=currentMin\n",
    "print(\"Parallel:main [LOG] Temperature:\", args.temperature, \"\\t Smallest marginal probability:\", smallestProb, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERR:init [INFO] RankingSize 10\n"
     ]
    }
   ],
   "source": [
    "metric=None\n",
    "if args.value_metric==\"DCG\":\n",
    "    metric=Metrics.DCG(data, args.length_ranking)\n",
    "elif args.value_metric==\"NDCG\":\n",
    "    metric=Metrics.NDCG(data, args.length_ranking, args.replacement)\n",
    "elif args.value_metric==\"ERR\":\n",
    "    metric=Metrics.ERR(data, args.length_ranking)\n",
    "elif args.value_metric==\"MaxRel\":\n",
    "    metric=Metrics.MaxRelevance(data, args.length_ranking)\n",
    "elif args.value_metric==\"SumRel\":\n",
    "    metric=Metrics.SumRelevance(data, args.length_ranking)\n",
    "else:\n",
    "    print(\"Parallel:main [ERR] Metric %s not supported.\" % args.value_metric, flush=True)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Estimators' from '/Users/haohongxu/slates_semisynth_expts/Estimators.py'>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Estimators\n",
    "import imp\n",
    "imp.reload(Estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Estimators\n",
    "import imp\n",
    "imp.reload(Estimators)\n",
    "estimator=None\n",
    "if args.approach==\"OnPolicy\":\n",
    "    estimator=Estimators.OnPolicy(args.length_ranking, loggingPolicy, targetPolicy, metric)\n",
    "    estimator.estimateAll()\n",
    "elif args.approach==\"IPS\":\n",
    "    if args.temperature > 0.0:\n",
    "        estimator=Estimators.NonUniformIPS_s(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "    else:    \n",
    "        estimator=Estimators.UniformIPS(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "elif args.approach==\"IPS_SN\":\n",
    "    if args.temperature > 0.0:\n",
    "        estimator=Estimators.NonUniformSNIPS(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "    else:\n",
    "        estimator=Estimators.UniformSNIPS(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "elif args.approach==\"PI\":\n",
    "    if args.temperature > 0.0:\n",
    "        estimator=Estimators.NonUniformPI(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "    else:\n",
    "        estimator=Estimators.UniformPI(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "elif args.approach==\"PI_SN\":\n",
    "    if args.temperature > 0.0:\n",
    "        estimator=Estimators.NonUniformSNPI(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "    else:\n",
    "        estimator=Estimators.UniformSNPI(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "elif args.approach.startswith(\"DM\"):\n",
    "    estimatorType=args.approach.split('_',1)[1]\n",
    "    estimator=Estimators.Direct(args.length_ranking, loggingPolicy, targetPolicy, estimatorType)\n",
    "else:\n",
    "    print(\"Parallel:main [ERR] Estimator %s not supported.\" % args.approach, flush=True)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Estimators.NonUniformIPS_s at 0x7fc04dd71490>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.length_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([12, 33, 22,  1, 37,  0, 13, 29, 36, 24])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targetPolicy.predict(i, args.length_ranking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................................................\n",
      "Parallel:main [LOG] *** TARGET:  0.18709950455856483\n"
     ]
    }
   ],
   "source": [
    "### IPS LOG\n",
    "import imp\n",
    "imp.reload(Policy)\n",
    "numQueries=len(data.docsPerQuery)\n",
    "trueMetric=numpy.zeros(numQueries, dtype=numpy.float64)\n",
    "\n",
    "for i in range(numQueries):\n",
    "    trueMetric[i]=metric.computeMetric(i, targetPolicy.predict(i, args.length_ranking))\n",
    "    if i%100==0:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "print(\"\", flush=True)\n",
    "\n",
    "target=trueMetric.mean(dtype=numpy.float64)\n",
    "print(\"Parallel:main [LOG] *** TARGET: \", target, flush = True)\n",
    "del trueMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    3,     6,     9, ..., 29994, 29997, 30000])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saveValues = numpy.linspace(start=int(args.logSize/args.saveSize), stop=args.logSize, num=args.saveSize, endpoint=True, dtype=numpy.int)\n",
    "saveValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputString = args.output_dir+'ssynth_'+args.value_metric+'_'+args.dataset+'_'+'4'\n",
    "if args.max_docs is None:\n",
    "    outputString += '-1_'\n",
    "else:\n",
    "    outputString += str(args.max_docs)+'_'\n",
    "\n",
    "outputString += str(args.length_ranking) +'_'\n",
    "if args.replacement:\n",
    "    outputString += 'r'\n",
    "else:\n",
    "    outputString += 'n'\n",
    "outputString += str(float(args.temperature)) + '_' \n",
    "outputString += 'f' + args.logging_ranker + '_e' + args.evaluation_ranker + '_' + str(args.numpy_seed)\n",
    "outputString += '_'+args.approach\n",
    "if args.approach.startswith(\"DM\"):\n",
    "    outputString += '_'+str(args.trainingSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Parallel:main [LOG] Iter:1 Truth Estimate=0.18979\n",
      "Parallel:main [LOG] IPS Estimate=0.18407 MSE=3.273e-05\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:2 Truth Estimate=0.18979\n",
      "Parallel:main [LOG] IPS Estimate=0.15963 MSE=9.096e-04\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:3 Truth Estimate=0.18979\n",
      "Parallel:main [LOG] IPS Estimate=0.24354 MSE=2.889e-03\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:4 Truth Estimate=0.18979\n",
      "Parallel:main [LOG] IPS Estimate=0.26935 MSE=6.329e-03\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:5 Truth Estimate=0.18979\n",
      "Parallel:main [LOG] IPS Estimate=0.19045 MSE=4.359e-07\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:6 Truth Estimate=0.18979\n",
      "Parallel:main [LOG] IPS Estimate=0.28963 MSE=9.967e-03\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:7 Truth Estimate=0.18979\n",
      "Parallel:main [LOG] IPS Estimate=0.31394 MSE=1.541e-02\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:8 Truth Estimate=0.18979\n",
      "Parallel:main [LOG] IPS Estimate=0.24513 MSE=3.062e-03\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:9 Truth Estimate=0.18979\n",
      "Parallel:main [LOG] IPS Estimate=0.24831 MSE=3.424e-03\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(args.start, args.stop):\n",
    "    iterOutputString = outputString+'_'+str(iteration)+'.z'\n",
    "    if os.path.isfile(iterOutputString):\n",
    "        print(\"Parallel:main [LOG] *** Found %s, skipping\" % iterOutputString, flush=True)\n",
    "        continue\n",
    "\n",
    "    # Reset estimator\n",
    "    estimator.reset()\n",
    "\n",
    "    # reset output\n",
    "    saveMSEs = numpy.zeros(args.saveSize, numpy.float64)\n",
    "    savePreds = numpy.zeros(args.saveSize, numpy.float64)\n",
    "\n",
    "    numpy.random.seed(args.numpy_seed + 7*iteration)\n",
    "    currentSaveIndex=0\n",
    "    currentSaveValue=saveValues[currentSaveIndex]-1\n",
    "\n",
    "    loggedData=None\n",
    "    if args.trainingSize > 0:\n",
    "        loggedData=[]\n",
    "\n",
    "    for j in range(args.logSize):\n",
    "        currentQuery=numpy.random.randint(0, numQueries)\n",
    "        loggedRanking=loggingPolicy.predict(currentQuery, args.length_ranking)\n",
    "        loggedValue=metric.computeMetric(currentQuery, loggedRanking)\n",
    "\n",
    "        newRanking=targetPolicy.predict(currentQuery,args.length_ranking)\n",
    "\n",
    "        estimatedValue=None\n",
    "        if (args.trainingSize > 0 and j < args.trainingSize):\n",
    "            estimatedValue=0.0\n",
    "            loggedData.append((currentQuery, loggedRanking, loggedValue))\n",
    "        else:\n",
    "            if j==args.trainingSize:\n",
    "                try:\n",
    "                    estimator.train(loggedData)\n",
    "                    if args.approach.startswith(\"DMc\"):\n",
    "                        estimator.estimateAll(metric=metric)\n",
    "                    else:\n",
    "                        estimator.estimateAll()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "            estimatedValue=estimator.estimate(currentQuery, loggedRanking, newRanking, loggedValue)/1000\n",
    "\n",
    "        if j==currentSaveValue:\n",
    "            savePreds[currentSaveIndex]=estimatedValue\n",
    "            saveMSEs[currentSaveIndex]=(estimatedValue-target)**2\n",
    "            currentSaveIndex+=1\n",
    "            if currentSaveIndex<args.saveSize:\n",
    "                currentSaveValue=saveValues[currentSaveIndex]-1\n",
    "\n",
    "        if j%1000==0:\n",
    "            print(\".\", end = \"\", flush = True)\n",
    "            numpy.random.seed(args.numpy_seed + 7*iteration + j + 1)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Parallel:main [LOG] Iter:%d Truth Estimate=%0.5f\" % (iteration, target), flush = True)\n",
    "    print(\"Parallel:main [LOG] %s Estimate=%0.5f MSE=%0.3e\" % (args.approach, savePreds[-1], saveMSEs[-1]), flush=True)\n",
    "\n",
    "    joblib.dump((saveValues, saveMSEs, savePreds, target), iterOutputString)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(saveMSEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change it to NDCG\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=None\n",
    "if args.value_metric==\"DCG\":\n",
    "    metric=Metrics.DCG(data, args.length_ranking)\n",
    "elif args.value_metric==\"NDCG\":\n",
    "    metric=Metrics.NDCG(data, args.length_ranking, args.replacement)\n",
    "elif args.value_metric==\"ERR\":\n",
    "    metric=Metrics.ERR(data, args.length_ranking)\n",
    "elif args.value_metric==\"MaxRel\":\n",
    "    metric=Metrics.MaxRelevance(data, args.length_ranking)\n",
    "elif args.value_metric==\"SumRel\":\n",
    "    metric=Metrics.SumRelevance(data, args.length_ranking)\n",
    "else:\n",
    "    print(\"Parallel:main [ERR] Metric %s not supported.\" % args.value_metric, flush=True)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IPS LOG\n",
    "numQueries=len(data.docsPerQuery)\n",
    "trueMetric=numpy.zeros(numQueries, dtype=numpy.float64)\n",
    "for i in range(numQueries):\n",
    "    trueMetric[i]=metric.computeMetric(i, targetPolicy.predict(i, args.length_ranking))\n",
    "    if i%100==0:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "print(\"\", flush=True)\n",
    "\n",
    "target=trueMetric.mean(dtype=numpy.float64)\n",
    "print(\"Parallel:main [LOG] *** TARGET: \", target, flush = True)\n",
    "del trueMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DM LOG\n",
    "numQueries=len(data.docsPerQuery)\n",
    "trueMetric=numpy.zeros(numQueries, dtype=numpy.float64)\n",
    "for i in range(numQueries):\n",
    "    trueMetric[i]=metric.computeMetric(i, targetPolicy.predict(i, args.length_ranking))\n",
    "    if i%100==0:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "print(\"\", flush=True)\n",
    "\n",
    "target=trueMetric.mean(dtype=numpy.float64)\n",
    "print(\"Parallel:main [LOG] *** TARGET: \", target, flush = True)\n",
    "del trueMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveValues = numpy.linspace(start=int(args.logSize/args.saveSize), stop=args.logSize, num=args.saveSize, endpoint=True, dtype=numpy.int)\n",
    "saveValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputString = args.output_dir+'ssynth_'+args.value_metric+'_'+args.dataset+'_'\n",
    "if args.max_docs is None:\n",
    "    outputString += '-1_'\n",
    "else:\n",
    "    outputString += str(args.max_docs)+'_'\n",
    "\n",
    "outputString += str(args.length_ranking) +'_'\n",
    "if args.replacement:\n",
    "    outputString += 'r'\n",
    "else:\n",
    "    outputString += 'n'\n",
    "outputString += str(float(args.temperature)) + '_' \n",
    "outputString += 'f' + args.logging_ranker + '_e' + args.evaluation_ranker + '_' + str(args.numpy_seed)\n",
    "outputString += '_'+args.approach\n",
    "if args.approach.startswith(\"DM\"):\n",
    "    outputString += '_'+str(args.trainingSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................\n",
      "Parallel:main [LOG] Iter:1 Truth Estimate=0.18710\n",
      "Parallel:main [LOG] IPS Estimate=243.02253 MSE=5.897e+04\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:2 Truth Estimate=0.18710\n",
      "Parallel:main [LOG] IPS Estimate=202.34533 MSE=4.087e+04\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:3 Truth Estimate=0.18710\n",
      "Parallel:main [LOG] IPS Estimate=219.37959 MSE=4.805e+04\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:4 Truth Estimate=0.18710\n",
      "Parallel:main [LOG] IPS Estimate=330.72815 MSE=1.093e+05\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:5 Truth Estimate=0.18710\n",
      "Parallel:main [LOG] IPS Estimate=303.82821 MSE=9.220e+04\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:6 Truth Estimate=0.18710\n",
      "Parallel:main [LOG] IPS Estimate=512.39066 MSE=2.624e+05\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:7 Truth Estimate=0.18710\n",
      "Parallel:main [LOG] IPS Estimate=321.73316 MSE=1.034e+05\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:8 Truth Estimate=0.18710\n",
      "Parallel:main [LOG] IPS Estimate=269.59349 MSE=7.258e+04\n",
      "..............................\n",
      "Parallel:main [LOG] Iter:9 Truth Estimate=0.18710\n",
      "Parallel:main [LOG] IPS Estimate=268.09983 MSE=7.178e+04\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(args.start, args.stop):\n",
    "    iterOutputString = outputString+'_'+str(iteration)+'.z'\n",
    "    if os.path.isfile(iterOutputString):\n",
    "        print(\"Parallel:main [LOG] *** Found %s, skipping\" % iterOutputString, flush=True)\n",
    "        continue\n",
    "\n",
    "    # Reset estimator\n",
    "    estimator.reset()\n",
    "\n",
    "    # reset output\n",
    "    saveMSEs = numpy.zeros(args.saveSize, numpy.float64)\n",
    "    savePreds = numpy.zeros(args.saveSize, numpy.float64)\n",
    "\n",
    "    numpy.random.seed(args.numpy_seed + 7*iteration)\n",
    "    currentSaveIndex=0\n",
    "    currentSaveValue=saveValues[currentSaveIndex]-1\n",
    "\n",
    "    loggedData=None\n",
    "    if args.trainingSize > 0:\n",
    "        loggedData=[]\n",
    "\n",
    "    for j in range(args.logSize):\n",
    "        currentQuery=numpy.random.randint(0, numQueries)\n",
    "        loggedRanking=loggingPolicy.predict(currentQuery, args.length_ranking)\n",
    "        loggedValue=metric.computeMetric(currentQuery, loggedRanking)\n",
    "\n",
    "        newRanking=targetPolicy.predict(currentQuery,args.length_ranking)\n",
    "\n",
    "        estimatedValue=None\n",
    "        if (args.trainingSize > 0 and j < args.trainingSize):\n",
    "            estimatedValue=0.0\n",
    "            loggedData.append((currentQuery, loggedRanking, loggedValue))\n",
    "        else:\n",
    "            if j==args.trainingSize:\n",
    "                try:\n",
    "                    estimator.train(loggedData)\n",
    "                    if args.approach.startswith(\"DMc\"):\n",
    "                        estimator.estimateAll(metric=metric)\n",
    "                    else:\n",
    "                        estimator.estimateAll()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "            estimatedValue=estimator.estimate(currentQuery, loggedRanking, newRanking, loggedValue)\n",
    "\n",
    "        if j==currentSaveValue:\n",
    "            savePreds[currentSaveIndex]=estimatedValue\n",
    "            saveMSEs[currentSaveIndex]=(estimatedValue-target)**2\n",
    "            currentSaveIndex+=1\n",
    "            if currentSaveIndex<args.saveSize:\n",
    "                currentSaveValue=saveValues[currentSaveIndex]-1\n",
    "\n",
    "        if j%1000==0:\n",
    "            print(\".\", end = \"\", flush = True)\n",
    "            numpy.random.seed(args.numpy_seed + 7*iteration + j + 1)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Parallel:main [LOG] Iter:%d Truth Estimate=%0.5f\" % (iteration, target), flush = True)\n",
    "    print(\"Parallel:main [LOG] %s Estimate=%0.5f MSE=%0.3e\" % (args.approach, savePreds[-1], saveMSEs[-1]), flush=True)\n",
    "\n",
    "    ##joblib.dump((saveValues, saveMSEs, savePreds, target), iterOutputString)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change target policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numpy.random.seed(387)\n",
    "detLogger=DeterministicPolicy(data, 'ridge')\n",
    "detLogger.train(anchorURLFeatures, 'url')\n",
    "\n",
    "detLogger.filterDataset(100)\n",
    "data=detLogger.dataset\n",
    "del detLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup target policy\n",
    "numpy.random.seed(args.numpy_seed)\n",
    "targetPolicy=DeterministicPolicy(data, 'ridge')\n",
    "targetPolicy.train(bodyTitleDocFeatures, 'body')\n",
    "targetPolicy.predictAll(args.length_ranking)\n",
    "\n",
    "loggingPolicy=None\n",
    "if args.temperature <= 0.0:\n",
    "    loggingPolicy=UniformPolicy(data, args.replacement)\n",
    "\n",
    "else:\n",
    "    underlyingPolicy=DeterministicPolicy(data, args.logging_ranker)\n",
    "    underlyingPolicy.train(anchorURLFeatures, 'url')\n",
    "    loggingPolicy=NonUniformPolicy(underlyingPolicy, data, args.replacement, args.temperature)\n",
    "\n",
    "loggingPolicy.setupGamma(args.length_ranking)\n",
    "\n",
    "smallestProb=1.0\n",
    "docSet=set(data.docsPerQuery)\n",
    "for i in docSet:\n",
    "    currentMin=None\n",
    "    if args.temperature > 0.0:\n",
    "        currentMin=numpy.amin(loggingPolicy.multinomials[i])\n",
    "    else:\n",
    "        currentMin=1.0/i\n",
    "    if currentMin < smallestProb:\n",
    "        smallestProb=currentMin\n",
    "print(\"Parallel:main [LOG] Temperature:\", args.temperature, \"\\t Smallest marginal probability:\", smallestProb, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Change it to random forest\n",
    "import Estimators\n",
    "import imp\n",
    "imp.reload(Estimators)\n",
    "estimator=None\n",
    "if args.approach==\"OnPolicy\":\n",
    "    estimator=Estimators.OnPolicy(args.length_ranking, loggingPolicy, targetPolicy, metric)\n",
    "    estimator.estimateAll()\n",
    "elif args.approach==\"IPS\":\n",
    "    if args.temperature > 0.0:\n",
    "        estimator=Estimators.NonUniformIPS(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "    else:    \n",
    "        estimator=Estimators.UniformIPS(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "elif args.approach==\"IPS_SN\":\n",
    "    if args.temperature > 0.0:\n",
    "        estimator=Estimators.NonUniformSNIPS(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "    else:\n",
    "        estimator=Estimators.UniformSNIPS(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "elif args.approach==\"PI\":\n",
    "    if args.temperature > 0.0:\n",
    "        estimator=Estimators.NonUniformPI(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "    else:\n",
    "        estimator=Estimators.UniformPI(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "elif args.approach==\"PI_SN\":\n",
    "    if args.temperature > 0.0:\n",
    "        estimator=Estimators.NonUniformSNPI(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "    else:\n",
    "        estimator=Estimators.UniformSNPI(args.length_ranking, loggingPolicy, targetPolicy)\n",
    "elif args.approach.startswith(\"DM\"):\n",
    "    estimatorType='rf'\n",
    "    estimator=Estimators.Direct(args.length_ranking, loggingPolicy, targetPolicy, estimatorType)\n",
    "else:\n",
    "    print(\"Parallel:main [ERR] Estimator %s not supported.\" % args.approach, flush=True)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=None\n",
    "if args.value_metric==\"DCG\":\n",
    "    metric=Metrics.DCG(data, args.length_ranking)\n",
    "elif args.value_metric==\"NDCG\":\n",
    "    metric=Metrics.NDCG(data, args.length_ranking, args.replacement)\n",
    "elif args.value_metric==\"ERR\":\n",
    "    metric=Metrics.ERR(data, args.length_ranking)\n",
    "elif args.value_metric==\"MaxRel\":\n",
    "    metric=Metrics.MaxRelevance(data, args.length_ranking)\n",
    "elif args.value_metric==\"SumRel\":\n",
    "    metric=Metrics.SumRelevance(data, args.length_ranking)\n",
    "else:\n",
    "    print(\"Parallel:main [ERR] Metric %s not supported.\" % args.value_metric, flush=True)\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### IPS LOG\n",
    "numQueries=len(data.docsPerQuery)\n",
    "trueMetric=numpy.zeros(numQueries, dtype=numpy.float64)\n",
    "for i in range(numQueries):\n",
    "    trueMetric[i]=metric.computeMetric(i, targetPolicy.predict(i, args.length_ranking))\n",
    "    if i%100==0:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "print(\"\", flush=True)\n",
    "\n",
    "target=trueMetric.mean(dtype=numpy.float64)\n",
    "print(\"Parallel:main [LOG] *** TARGET: \", target, flush = True)\n",
    "del trueMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DM LOG\n",
    "numQueries=len(data.docsPerQuery)\n",
    "trueMetric=numpy.zeros(numQueries, dtype=numpy.float64)\n",
    "for i in range(numQueries):\n",
    "    trueMetric[i]=metric.computeMetric(i, targetPolicy.predict(i, args.length_ranking))\n",
    "    if i%100==0:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "print(\"\", flush=True)\n",
    "\n",
    "target=trueMetric.mean(dtype=numpy.float64)\n",
    "print(\"Parallel:main [LOG] *** TARGET: \", target, flush = True)\n",
    "del trueMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveValues = numpy.linspace(start=int(args.logSize/args.saveSize), stop=args.logSize, num=args.saveSize, endpoint=True, dtype=numpy.int)\n",
    "saveValues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputString = args.output_dir+'ssynth_'+args.value_metric+'_'+args.dataset+'_'+targetPolicy.name\n",
    "if args.max_docs is None:\n",
    "    outputString += '-1_'\n",
    "else:\n",
    "    outputString += str(args.max_docs)+'_'\n",
    "\n",
    "outputString += str(args.length_ranking) +'_'\n",
    "if args.replacement:\n",
    "    outputString += 'r'\n",
    "else:\n",
    "    outputString += 'n'\n",
    "outputString += str(float(args.temperature)) + '_' \n",
    "outputString += 'f' + args.logging_ranker + '_e' + args.evaluation_ranker + '_' + str(args.numpy_seed)\n",
    "outputString += '_'+args.approach\n",
    "if args.approach.startswith(\"DM\"):\n",
    "    outputString += '_'+str(args.trainingSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(args.start, args.stop):\n",
    "    iterOutputString = outputString+'_'+str(iteration)+'.z'\n",
    "    if os.path.isfile(iterOutputString):\n",
    "        print(\"Parallel:main [LOG] *** Found %s, skipping\" % iterOutputString, flush=True)\n",
    "        continue\n",
    "\n",
    "    # Reset estimator\n",
    "    estimator.reset()\n",
    "\n",
    "    # reset output\n",
    "    saveMSEs = numpy.zeros(args.saveSize, numpy.float64)\n",
    "    savePreds = numpy.zeros(args.saveSize, numpy.float64)\n",
    "\n",
    "    numpy.random.seed(args.numpy_seed + 7*iteration)\n",
    "    currentSaveIndex=0\n",
    "    currentSaveValue=saveValues[currentSaveIndex]-1\n",
    "\n",
    "    loggedData=None\n",
    "    if args.trainingSize > 0:\n",
    "        loggedData=[]\n",
    "\n",
    "    for j in range(args.logSize):\n",
    "        currentQuery=numpy.random.randint(0, numQueries)\n",
    "        loggedRanking=loggingPolicy.predict(currentQuery, args.length_ranking)\n",
    "        loggedValue=metric.computeMetric(currentQuery, loggedRanking)\n",
    "\n",
    "        newRanking=targetPolicy.predict(currentQuery,args.length_ranking)\n",
    "\n",
    "        estimatedValue=None\n",
    "        if (args.trainingSize > 0 and j < args.trainingSize):\n",
    "            estimatedValue=0.0\n",
    "            loggedData.append((currentQuery, loggedRanking, loggedValue))\n",
    "        else:\n",
    "            if j==args.trainingSize:\n",
    "                try:\n",
    "                    estimator.train(loggedData)\n",
    "                    if args.approach.startswith(\"DMc\"):\n",
    "                        estimator.estimateAll(metric=metric)\n",
    "                    else:\n",
    "                        estimator.estimateAll()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "            estimatedValue=estimator.estimate(currentQuery, loggedRanking, newRanking, loggedValue)\n",
    "\n",
    "        if j==currentSaveValue:\n",
    "            savePreds[currentSaveIndex]=estimatedValue\n",
    "            saveMSEs[currentSaveIndex]=(estimatedValue-target)**2\n",
    "            currentSaveIndex+=1\n",
    "            if currentSaveIndex<args.saveSize:\n",
    "                currentSaveValue=saveValues[currentSaveIndex]-1\n",
    "\n",
    "        if j%1000==0:\n",
    "            print(\".\", end = \"\", flush = True)\n",
    "            numpy.random.seed(args.numpy_seed + 7*iteration + j + 1)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Parallel:main [LOG] Iter:%d Truth Estimate=%0.5f\" % (iteration, target), flush = True)\n",
    "    print(\"Parallel:main [LOG] %s Estimate=%0.5f MSE=%0.3e\" % ('DM_rf', savePreds[-1], saveMSEs[-1]), flush=True)\n",
    "\n",
    "    ##joblib.dump((saveValues, saveMSEs, savePreds, target), iterOutputString)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
